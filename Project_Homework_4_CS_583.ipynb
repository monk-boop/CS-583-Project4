{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project Homework 4 CS 583.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JirHSHTyCs-"
      },
      "source": [
        "Name: \n",
        "- Mayank Khanna ( CWID:)\n",
        "- Patrick Simmon ( CWID:)\n",
        "\n",
        "Note: We Will be attempting the full 20 bonus points that are provided in this assignment.\n",
        "\n",
        "<H1>Total Attempt: 10 + 20  = 30</H1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ln_6oZFKH-7"
      },
      "source": [
        "<h1>3. Intent Classificantion using SVM(CS 583 Fall 2021)</h1>\n",
        "<h2> DataSet used : <a href=\"https://github.com/clinc/oos-eval\">CLINC150 </a>\n",
        "\n",
        "<h2> Objective: </h2>\n",
        "<ul>\n",
        "  <li> Use text data to classify (map) text to a particular intent. This dataset contains 150 intents in total and our objective requires us to map the text to a particular intent. </li>\n",
        "  <li> This generally finds applications in chatbots where a query from a user is routed to a particular customer service team. If this chatbot is not a part of a ciritcal application, then both false postitive and false negitives are acceptable. However, while building any model, we should strive to reduce false positives and negitives to a minimum. We should keep in mind that an incorrect classification from a model can hurt customer satisfaction for a company.</li>\n",
        "  <li> For the purpose of this assignment, we will test out model on accuracy, since the applications are low priority and the daataset is almost balanced. </li>\n",
        "</ul>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QewRs-gM6xQ"
      },
      "source": [
        "<h3> Data assumptions and structure. </h3>\n",
        "\n",
        "The data file is called 'data_full.json' and is stored in the same directory as this notebook. The structure of the dataset is as follows:\n",
        "```\n",
        "Data Format\n",
        "{\n",
        "\n",
        "  \"train\": List of training samples with each sample representing a tuple: (query,class)\n",
        "          [\n",
        "               [\"query / sentence\",\"intent_class\"],\n",
        "               [\"query / sentence\",\"intent_class\"],\n",
        "          ],\n",
        "  \"val\":  List of validation samples with each sample representing a tuple: (query,class)\n",
        "          [\n",
        "               [\"query / sentence\",\"intent_class\"],\n",
        "               [\"query / sentence\",\"intent_class\"],\n",
        "          ],\n",
        "   \"test\":  List of testing samples with each sample representing a tuple: (query,class)\n",
        "          [\n",
        "               [\"query / sentence\",\"intent_class\"],\n",
        "               [\"query / sentence\",\"intent_class\"],\n",
        "          ],\n",
        "    ...further similar 3 sets for out-of-scope queries for advance classification.\n",
        "}\n",
        "```\n",
        "  \n",
        "\n",
        "<ul>\n",
        "  <li> Since we are given a choice to either test accuracy on the 150 intents that are provided OR the recall on oos queries, we have decided to proceed with the accuracy of intents on the dataset provided. </li>\n",
        "</ul>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3osA8huOkvD"
      },
      "source": [
        "<h3>Approach</h3>\n",
        "<ul>\n",
        "<li> We first import the data and clean it, featurize it and build models on it. Follow along the notebook to understand our approach!</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iRg4XgLx8DE"
      },
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWpdEbP1ykmL"
      },
      "source": [
        "with open('data_full.json') as json_file: \n",
        "    data_dict = json.load(json_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWOOjxhaypdF"
      },
      "source": [
        "#Since the train, val and test datasets are already divided, we just directly load them\n",
        "train_data = data_dict['train']\n",
        "val_data = data_dict['val']\n",
        "test_data = data_dict['test']\n",
        "\n",
        "train_df = pd.DataFrame(train_data, columns =['query', 'intent'])\n",
        "val_df = pd.DataFrame(val_data, columns =['query', 'intent'])\n",
        "test_df = pd.DataFrame(test_data, columns =['query', 'intent'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8rJ8QHDQO27"
      },
      "source": [
        "<h4>Check the percentage of target values in train, val and test to see if the dataset is imbalanced or not.</h4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6nrHU-GQBNL",
        "outputId": "1ad9148e-2f99-44f6-f51d-3d9b71216896"
      },
      "source": [
        "#train data\n",
        "list(train_df['intent'].value_counts(normalize=True) * 100)\n",
        "train_df['intent'].value_counts(normalize=True) * 100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "schedule_maintenance    0.666667\n",
              "cancel                  0.666667\n",
              "share_location          0.666667\n",
              "goodbye                 0.666667\n",
              "flight_status           0.666667\n",
              "                          ...   \n",
              "calories                0.666667\n",
              "reminder                0.666667\n",
              "oil_change_how          0.666667\n",
              "spending_history        0.666667\n",
              "sync_device             0.666667\n",
              "Name: intent, Length: 150, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8a5MUr-0shX",
        "outputId": "220d2288-e8c1-4653-834b-0d537f28fe7b"
      },
      "source": [
        "#test data\n",
        "list(test_df['intent'].value_counts(normalize=True) * 100)\n",
        "test_df['intent'].value_counts(normalize=True) * 100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "what_can_i_ask_you      0.666667\n",
              "carry_on                0.666667\n",
              "what_is_your_name       0.666667\n",
              "report_lost_card        0.666667\n",
              "shopping_list_update    0.666667\n",
              "                          ...   \n",
              "update_playlist         0.666667\n",
              "account_blocked         0.666667\n",
              "taxes                   0.666667\n",
              "lost_luggage            0.666667\n",
              "how_old_are_you         0.666667\n",
              "Name: intent, Length: 150, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lplt51sQip2",
        "outputId": "0b7853ce-858e-485b-ce00-e250fd42d4af"
      },
      "source": [
        "#Val data\n",
        "list(val_df['intent'].value_counts(normalize=True) * 100)\n",
        "val_df['intent'].value_counts(normalize=True) * 100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "what_can_i_ask_you      0.666667\n",
              "carry_on                0.666667\n",
              "what_is_your_name       0.666667\n",
              "report_lost_card        0.666667\n",
              "shopping_list_update    0.666667\n",
              "                          ...   \n",
              "update_playlist         0.666667\n",
              "account_blocked         0.666667\n",
              "taxes                   0.666667\n",
              "lost_luggage            0.666667\n",
              "how_old_are_you         0.666667\n",
              "Name: intent, Length: 150, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjMu9ZVrQpZK"
      },
      "source": [
        "<h4>We observe that all the target values are equally balanced in train, val, test!</h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1GDC7_cQ4Uj"
      },
      "source": [
        "<h3>Number of Datapoints in train, val, test</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wx6AtCy21GEC",
        "outputId": "65727112-a76b-461d-e503-5d5bb7e9d8e7"
      },
      "source": [
        "visited = []\n",
        "cnt = 0\n",
        "for i in range(0, len(train_df['intent'])):\n",
        "    if train_df['intent'][i] not in visited: \n",
        "        visited.append(train_df['intent'][i])\n",
        "        cnt += 1\n",
        "\n",
        "print(\"Number of data points in train data\", train_df.shape)\n",
        "print('-'*50)\n",
        "print(\"The attributes of data :\", train_df.columns.values)\n",
        "print('-'*50)\n",
        "print(\"The number of intents in train dataset is\",cnt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of data points in train data (15000, 2)\n",
            "--------------------------------------------------\n",
            "The attributes of data : ['query' 'intent']\n",
            "--------------------------------------------------\n",
            "The number of intents in train dataset is 150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSO-XErDRFyx",
        "outputId": "44e807cd-54b4-4895-f508-8c91488d7673"
      },
      "source": [
        "visited = []\n",
        "cnt = 0\n",
        "for i in range(0, len(test_df['intent'])):\n",
        "    if test_df['intent'][i] not in visited: \n",
        "        visited.append(test_df['intent'][i])\n",
        "        cnt += 1\n",
        "\n",
        "print(\"Number of data points in test data\", test_df.shape)\n",
        "print('-'*50)\n",
        "print(\"The attributes of data :\", test_df.columns.values)\n",
        "print('-'*50)\n",
        "print(\"The number of intents in test dataset is\",cnt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of data points in test data (4500, 2)\n",
            "--------------------------------------------------\n",
            "The attributes of data : ['query' 'intent']\n",
            "--------------------------------------------------\n",
            "The number of intents in test dataset is 150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FA87q5HoRPTu",
        "outputId": "435f427c-e7bb-47c9-807c-7ec86073eb5a"
      },
      "source": [
        "visited = []\n",
        "cnt = 0\n",
        "for i in range(0, len(val_df['intent'])):\n",
        "    if val_df['intent'][i] not in visited: \n",
        "        visited.append(val_df['intent'][i])\n",
        "        cnt += 1\n",
        "\n",
        "print(\"Number of data points in val data\", val_df.shape)\n",
        "print('-'*50)\n",
        "print(\"The attributes of data :\", val_df.columns.values)\n",
        "print('-'*50)\n",
        "print(\"The number of intents in val dataset is\",cnt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of data points in val data (3000, 2)\n",
            "--------------------------------------------------\n",
            "The attributes of data : ['query' 'intent']\n",
            "--------------------------------------------------\n",
            "The number of intents in val dataset is 150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32MljPLg1rfX",
        "outputId": "e7b41cdc-faa5-4bc2-a97b-742902aad8d4"
      },
      "source": [
        "## Printing some random query points from the train dataset\n",
        "print(train_df['query'].values[0])\n",
        "print(\"=\"*50)\n",
        "print(train_df['query'].values[3000])\n",
        "print(\"=\"*50)\n",
        "print(train_df['query'].values[6000])\n",
        "print(\"=\"*50)\n",
        "print(train_df['query'].values[9000])\n",
        "print(\"=\"*50)\n",
        "print(train_df['query'].values[12099])\n",
        "print(\"=\"*50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "what expression would i use to say i love you if i were an italian\n",
            "==================================================\n",
            "estimated time to airport from current location, la\n",
            "==================================================\n",
            "if i fly american to los angeles, how many carry ons am i allowed\n",
            "==================================================\n",
            "who is your employeer\n",
            "==================================================\n",
            "do you have a name i should call you by\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CT9geiJRifk"
      },
      "source": [
        "<h2>We will now begin with data cleaning</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9xKFHzgTMqi"
      },
      "source": [
        "<h4>We see in the dataset that a lot of contractions are present. Contractions are words like What're / What's /I'm that can be expanded into words like What are/ what is / I am. For good data cleaning, we first take care of all these contractions and expand them to their full forms</h4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dczlkcQ3-ljZ"
      },
      "source": [
        "# Reference : https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python \n",
        "def decontracted(phrase):\n",
        "    # specific\n",
        "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    \n",
        "    return phrase\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jw62tJEfhIPf"
      },
      "source": [
        "<h4>We now remove stopwords. Stopwords are very common words in the english language that don't add any value or meaning to the sentence.</h4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLbgW5X0_Ujj"
      },
      "source": [
        "#stopword removal\n",
        "stopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
        "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
        "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
        "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
        "            'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
        "            'did', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
        "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
        "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
        "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
        "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
        "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
        "            've', 'y', 'ain']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRFkYiL2AL2-"
      },
      "source": [
        "preprocessed_query_train = []\n",
        "# tqdm is for printing the status bar\n",
        "for sentance in train_df['query'].values:\n",
        "    sent = decontracted(sentance)\n",
        "    #Replace characters with \\ http://texthandler.com/info/remove-line-breaks-python/\n",
        "    #sent = sent.replace('\\\\r', ' ')\n",
        "    #sent = sent.replace('\\\\\"', ' ')\n",
        "    #sent = sent.replace('\\\\n', ' ')\n",
        "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
        "    # https://gist.github.com/sebleier/554280\n",
        "    sent = ' '.join(e for e in sent.split() if e not in stopwords)#train_df['intent'].values[i]\n",
        "    # remove spacial character: https://stackoverflow.com/a/5843547/4084039\n",
        "    #make everything lower and strip excess spaces\n",
        "    preprocessed_query_train.append(sent.lower().strip())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAGhwDFZBNHp"
      },
      "source": [
        "train_df.drop(['query'],axis = 1,inplace = True)\n",
        "train_df['query'] = preprocessed_query_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_n8SHwAA1Wt"
      },
      "source": [
        "preprocessed_query_test = []\n",
        "# tqdm is for printing the status bar\n",
        "for sentance in test_df['query'].values:\n",
        "    sent = decontracted(sentance)\n",
        "    #Replace characters with \\ http://texthandler.com/info/remove-line-breaks-python/\n",
        "    sent = sent.replace('\\\\r', ' ')\n",
        "    sent = sent.replace('\\\\\"', ' ')\n",
        "    sent = sent.replace('\\\\n', ' ')\n",
        "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
        "    # https://gist.github.com/sebleier/554280\n",
        "    sent = ' '.join(e for e in sent.split() if e not in stopwords)\n",
        "    # remove spacial character: https://stackoverflow.com/a/5843547/4084039\n",
        "    #make everything lower and strip excess spaces\n",
        "    preprocessed_query_test.append(sent.lower().strip())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49XyHgvqBOFp"
      },
      "source": [
        "test_df.drop(['query'],axis = 1,inplace = True)\n",
        "test_df['query'] = preprocessed_query_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIieQRTlA8Wq"
      },
      "source": [
        "preprocessed_query_val = []\n",
        "# tqdm is for printing the status bar\n",
        "for sentance in val_df['query'].values:\n",
        "    sent = decontracted(sentance)\n",
        "    #Replace characters with \\ http://texthandler.com/info/remove-line-breaks-python/\n",
        "    sent = sent.replace('\\\\r', ' ')\n",
        "    sent = sent.replace('\\\\\"', ' ')\n",
        "    sent = sent.replace('\\\\n', ' ')\n",
        "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
        "    # https://gist.github.com/sebleier/554280\n",
        "    sent = ' '.join(e for e in sent.split() if e not in stopwords)\n",
        "    # remove spacial character: https://stackoverflow.com/a/5843547/4084039\n",
        "    #make everything lower and strip excess spaces\n",
        "    preprocessed_query_val.append(sent.lower().strip())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Om9n2heqBO9l"
      },
      "source": [
        "val_df.drop(['query'],axis = 1,inplace = True)\n",
        "val_df['query'] = preprocessed_query_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqNb_T1HB8Gd",
        "outputId": "0f239066-7c7b-4605-fd5b-935a9ba2cf02"
      },
      "source": [
        "## Printing some random query points from the train dataset\n",
        "\n",
        "print(train_df['query'].values[0])\n",
        "print(\"=\"*50)\n",
        "print(train_df['query'].values[3000])\n",
        "print(\"=\"*50)\n",
        "print(train_df['query'].values[6000])\n",
        "print(\"=\"*50)\n",
        "print(train_df['query'].values[9000])\n",
        "print(\"=\"*50)\n",
        "print(train_df['query'].values[12099])\n",
        "print(\"=\"*50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "expression would use say love italian\n",
            "==================================================\n",
            "estimated time airport current location la\n",
            "==================================================\n",
            "fly american los angeles many carry ons am allowed\n",
            "==================================================\n",
            "employeer\n",
            "==================================================\n",
            "name call\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IT3JM-q8hs-Z"
      },
      "source": [
        "<h4> Looking at the sentences above, we see that a lot of words were removed from the query. This can be bad because if the query consists of only stopwords, than the entire sentence will be removed. Let's check if this happened!</h4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8gpp-uJCayf",
        "outputId": "817aa720-44b0-4014-d2aa-1287efd4720f"
      },
      "source": [
        "#Check after cleaning, any query was removed or replaced with an empty string\n",
        "print(\"\" in preprocessed_query_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhmkFTXwiI9V"
      },
      "source": [
        "<h4>As expected, the queries were deleted entirely after cleaning. Let's investigate this further</h4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVllQJ_iCvR2"
      },
      "source": [
        "#Find the indices where the queries are empty after cleaning.\n",
        "empty_indices = [i for i,x in enumerate(preprocessed_query_train) if not x]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B33OleD9DBX4",
        "outputId": "468ede76-29cb-44bf-f288-5ffe5e76705d"
      },
      "source": [
        "original_train_df = pd.DataFrame(train_data, columns =['query', 'intent'])\n",
        "for i in empty_indices:\n",
        "  print(\"Original Query: \",original_train_df['query'].values[i])\n",
        "  print(\"Query after cleaning: \",train_df['query'].values[i])\n",
        "  print(\"Intent: \",train_df['intent'].values[i])\n",
        "  print(end = '\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Query:  where are you from\n",
            "Query:  \n",
            "Intent:  where_are_you_from\n",
            "\n",
            "Original Query:  what all can you do\n",
            "Query:  \n",
            "Intent:  what_can_i_ask_you\n",
            "\n",
            "Original Query:  what can you do\n",
            "Query:  \n",
            "Intent:  what_can_i_ask_you\n",
            "\n",
            "Original Query:  what was that\n",
            "Query:  \n",
            "Intent:  repeat\n",
            "\n",
            "Original Query:  do that\n",
            "Query:  \n",
            "Intent:  yes\n",
            "\n",
            "Original Query:  when will i be off again\n",
            "Query:  \n",
            "Intent:  next_holiday\n",
            "\n",
            "Original Query:  what's up\n",
            "Query:  \n",
            "Intent:  greeting\n",
            "\n",
            "Original Query:  how have you been\n",
            "Query:  \n",
            "Intent:  greeting\n",
            "\n",
            "Original Query:  how are you\n",
            "Query:  \n",
            "Intent:  greeting\n",
            "\n",
            "Original Query:  what's up with you\n",
            "Query:  \n",
            "Intent:  greeting\n",
            "\n",
            "Original Query:  what are you\n",
            "Query:  \n",
            "Intent:  are_you_a_bot\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EHWVpLoig52"
      },
      "source": [
        "<h4> From the code above, we can see that the queries that consist of only stopwords were removed and they all belong to different intents. Had they been sourced from the same intent, we could proceed with this since that would improve the accuracy of the model. However, since the queries belong to different intents here, we will not remove any stopwords</h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0uHY7azjyd5"
      },
      "source": [
        "<h2> Cleaning the data wothout removing any stopwords</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1H23jJ2fDTky"
      },
      "source": [
        "train_df = pd.DataFrame(train_data, columns =['query', 'intent'])\n",
        "val_df = pd.DataFrame(val_data, columns =['query', 'intent'])\n",
        "test_df = pd.DataFrame(test_data, columns =['query', 'intent'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWUdYJ0Fj7vC"
      },
      "source": [
        "preprocessed_query_train = []\n",
        "# tqdm is for printing the status bar\n",
        "for sentance in train_df['query'].values:\n",
        "    sent = decontracted(sentance)\n",
        "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
        "    # https://gist.github.com/sebleier/554280\n",
        "    sent = ' '.join(e for e in sent.split())#train_df['intent'].values[i]\n",
        "    # remove spacial character: https://stackoverflow.com/a/5843547/4084039\n",
        "    #make everything lower and strip excess spaces\n",
        "    preprocessed_query_train.append(sent.lower().strip())\\\n",
        "\n",
        "\n",
        "train_df.drop(['query'],axis = 1,inplace = True)\n",
        "train_df['query'] = preprocessed_query_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZ2UWb4fkIQI"
      },
      "source": [
        "preprocessed_query_test = []\n",
        "# tqdm is for printing the status bar\n",
        "for sentance in test_df['query'].values:\n",
        "    sent = decontracted(sentance)\n",
        "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
        "    # https://gist.github.com/sebleier/554280\n",
        "    sent = ' '.join(e for e in sent.split())\n",
        "    # remove spacial character: https://stackoverflow.com/a/5843547/4084039\n",
        "    #make everything lower and strip excess spaces\n",
        "    preprocessed_query_test.append(sent.lower().strip())\n",
        "\n",
        "test_df.drop(['query'],axis = 1,inplace = True)\n",
        "test_df['query'] = preprocessed_query_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMeS5Y90kRr-"
      },
      "source": [
        "preprocessed_query_val = []\n",
        "# tqdm is for printing the status bar\n",
        "for sentance in val_df['query'].values:\n",
        "    sent = decontracted(sentance)\n",
        "    #Replace characters with \\ http://texthandler.com/info/remove-line-breaks-python/\n",
        "    #sent = sent.replace('\\\\r', ' ')\n",
        "    #sent = sent.replace('\\\\\"', ' ')\n",
        "    #sent = sent.replace('\\\\n', ' ')\n",
        "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
        "    # https://gist.github.com/sebleier/554280\n",
        "    sent = ' '.join(e for e in sent.split())\n",
        "    # remove spacial character: https://stackoverflow.com/a/5843547/4084039\n",
        "    #make everything lower and strip excess spaces\n",
        "    preprocessed_query_val.append(sent.lower().strip())\n",
        "\n",
        "val_df.drop(['query'],axis = 1,inplace = True)\n",
        "val_df['query'] = preprocessed_query_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUpSePL1kxUp",
        "outputId": "3d0f8c08-9387-408d-e158-a778ceeab27c"
      },
      "source": [
        "# Check if any of the queries were removed after cleaning\n",
        "print(\"\" in preprocessed_query_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsbX4RO8k6_5",
        "outputId": "f6fd981d-d331-4fae-d992-ae41595d603e"
      },
      "source": [
        "print(\"\" in preprocessed_query_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsBgLhaik9RY",
        "outputId": "9349c1ee-be28-4af9-dde6-b28095b166f3"
      },
      "source": [
        "print(\"\" in preprocessed_query_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-lYt10KlBoe"
      },
      "source": [
        "<h4>Great! It looks like none of the queries were removed!</h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQUgFvL0lVdb"
      },
      "source": [
        "<h1>To Do: Try stemming and lemmitization</h1>\n",
        "<h1>To Do: Data featurization - Bag of Words(unigram, bi gram), Tfidf, W2V, Tfidf weighted W2v</h1>\n",
        "<h1>To Do: Model Building - SVM(main) with hyperparameter tuning using grid search(need to get accuracy above 88 at any cost), Logistic regression, naive bayes(maybe), Decision trees, random forests, XG boost, Stacking classifier, cascading classifier, Neural Networks</h1>"
      ]
    }
  ]
}